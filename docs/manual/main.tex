%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% cuRAMSES-kjhan Technical Reference Manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book}

\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,headheight=14pt,a4paper]{geometry}

\usepackage{xcolor}
\definecolor{ocre}{RGB}{52,177,201}

% Font Settings
\usepackage{avant}
\usepackage{mathptmx}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Bibliography
\usepackage[style=alphabetic,sorting=nyt,sortcites=true,autopunct=true,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
\addbibresource{bibliography.bib}
\defbibheading{bibempty}{}

\input{structure}

%----------------------------------------------------------------------------------------
%    Definitions of new commands
%----------------------------------------------------------------------------------------

\def\R{\mathbb{R}}

%------------------------------------------------------
% Document
%------------------------------------------------------
\begin{document}

%------------------------------------------------------
% Title page
%------------------------------------------------------
\begin{titlepage}
\begin{tikzpicture}[remember picture,overlay]
  % Top colored area
  \fill[ocre] (current page.north west) rectangle ([yshift=-9cm]current page.north east);
  % Bottom accent strip
  \fill[ocre!10] (current page.south west) rectangle ([yshift=2.5cm]current page.south east);
  \draw[ocre,line width=1.5pt] ([yshift=2.5cm]current page.south west) -- ([yshift=2.5cm]current page.south east);
\end{tikzpicture}

\vspace*{1cm}

\begin{center}
{\fontsize{42}{50}\selectfont\sffamily\bfseries\color{white}cuRAMSES-kjhan}\\[1cm]
{\fontsize{20}{24}\selectfont\sffamily\color{white!80}Technical Reference Manual}
\end{center}

\vspace{5cm}

\begin{center}
{\Large\sffamily
K-Section Ordering, Morton Key Octree,\\[8pt]
Memory-Based Load Balancing, and\\[8pt]
Performance Optimizations}

\vspace{3cm}

{\large\sffamily Jaehyun~Han}\\[0.5cm]
{\sffamily February 2026}
\end{center}

\vfill

\begin{center}
{\small\sffamily
Based on RAMSES (R.~Teyssier)\\[3pt]
\texttt{git@github.com:kjhan0606/cuRAMSES-kjhan.git}}
\end{center}
\vspace{1cm}
\end{titlepage}

%------------------------------------------------------
% Table of Contents
%------------------------------------------------------

\pagestyle{empty}
\tableofcontents
\pagestyle{fancy}

%------------------------------------------------------
% PART I: Modifications Overview
%------------------------------------------------------
\part{Code Modifications}

%======================================================
\chapter{Quick Start Guide}
\label{ch:quickstart}
%======================================================

This chapter provides a practical step-by-step guide to building cuRAMSES and running a cosmological zoom-in simulation. For detailed explanations of the underlying algorithms and parameters, refer to the subsequent chapters.

\section{Prerequisites}

\begin{itemize}[leftmargin=2em]
  \item \textbf{Intel Fortran Compiler} (\texttt{ifx} or \texttt{ifort}) with MPI wrapper (\texttt{mpiifx})
  \item \textbf{OpenMP} support (enabled via \texttt{-qopenmp})
  \item \textbf{MUSIC} --- Multi-Scale Initial Conditions generator (\texttt{https://bitbucket.org/ohahn/music})
  \item \textbf{Python 3} with \texttt{numpy} (for utility scripts such as \texttt{create\_pvar006.py})
\end{itemize}

\section{Building cuRAMSES}

\begin{codebox}[Build Commands]
\begin{lstlisting}[style=bash,numbers=none]
cd bin
make clean
make
\end{lstlisting}
\end{codebox}

The binary is produced as \texttt{bin/ramses\_final3d}. Key compile-time options (\texttt{-DNVAR=11}, \texttt{-DNPRE=8}, \texttt{-DLONGINT}, etc.) are set in the Makefile. See Chapter~\ref{ch:build} for details on compile-time flags.

\section{Setting Up a Zoom-In Simulation}

The following steps walk through a complete zoom-in cosmological simulation with star formation, targeting ${\sim}1$\,kpc resolution in a 10\,$h^{-1}$\,Mpc box.

\subsection{Step 1: Generate Initial Conditions with MUSIC}

Create a MUSIC configuration file. The key settings for a zoom-in are \texttt{levelmin} (base grid), \texttt{levelmax} (finest zoom level), and \texttt{ref\_extent} (zoom region fraction).

\begin{codebox}[zoomin\_10Mpc.conf (excerpt)]
\begin{lstlisting}[style=bash,numbers=none]
[setup]
boxlength       = 10          # Box size in Mpc/h
zstart          = 50          # Starting redshift
levelmin        = 7           # Base grid: 128^3
levelmax        = 11          # Zoom finest: 2048^3 effective
ref_center      = 0.5, 0.5, 0.5
ref_extent      = 0.04, 0.04, 0.04
baryons         = yes
use_2LPT        = yes

[cosmology]
Omega_m         = 0.3111
Omega_L         = 0.6889
Omega_b         = 0.04
H0              = 67.66
sigma_8         = 0.8102
nspec           = 0.9665
transfer        = eisenstein

[output]
format          = grafic2
filename        = IC_zoomin
\end{lstlisting}
\end{codebox}

Run MUSIC:
\begin{codebox}[Generate ICs]
\begin{lstlisting}[style=bash,numbers=none]
./MUSIC zoomin_10Mpc.conf
\end{lstlisting}
\end{codebox}

This produces \texttt{IC\_zoomin/level\_007} through \texttt{IC\_zoomin/level\_011}, each containing GRAFIC2 binary files (\texttt{ic\_deltab}, \texttt{ic\_velcx/y/z}, \texttt{ic\_poscx/y/z}, etc.).

\subsection{Step 2: Create Zoom Geometry Scalar}

The zoom geometry scalar (\texttt{ic\_pvar\_00006}) is a passive variable that marks which cells belong to the zoom region. This is essential when using \texttt{ivar\_refine=11} to control AMR refinement during initialization. See Chapter~\ref{ch:zoomin} for a detailed explanation.

\begin{codebox}[Create pvar006]
\begin{lstlisting}[style=bash,numbers=none]
cd test_ksection
python3 create_pvar006.py
\end{lstlisting}
\end{codebox}

Edit the script's \texttt{IC\_DIR} and \texttt{LEVELS} variables to match your IC directory.

\subsection{Step 3: Create Run Directory}

\begin{codebox}[Setup Run Directory]
\begin{lstlisting}[style=bash,numbers=none]
mkdir -p run_zoomin
cd run_zoomin
ln -s ../IC_zoomin .
cp ../../bin/ramses_final3d .
cp ../cosmo_zoomin_physics.nml namelist.nml
\end{lstlisting}
\end{codebox}

\subsection{Step 4: Configure Namelist}

The most important parameters to set correctly in the namelist:

\begin{codebox}[Key namelist parameters]
\begin{lstlisting}[style=fortran,numbers=none]
&RUN_PARAMS
ordering='ksection'     ! hierarchical domain decomposition
memory_balance=.true.   ! memory-weighted load balancing
nremap=5                ! load balance every 5 coarse steps
/

&AMR_PARAMS
levelmin=7              ! must match MUSIC levelmin
levelmax=20             ! maximum AMR level allowed
ngridtot=200000000      ! total grids (see memory sizing)
nparttot=200000000      ! total particles
/

&REFINE_PARAMS
m_refine=13*8.          ! Lagrangian refinement threshold
ivar_refine=11          ! use passive scalar for zoom mask
var_cut_refine=0.01     ! threshold for zoom scalar
mass_cut_refine=-1      ! set per IC level (see table below)
/

&INIT_PARAMS
filetype='grafic'
initfile(1)='IC_zoomin/level_007'
initfile(2)='IC_zoomin/level_008'
initfile(3)='IC_zoomin/level_009'
initfile(4)='IC_zoomin/level_010'
initfile(5)='IC_zoomin/level_011'
/
\end{lstlisting}
\end{codebox}

\subsection{Step 5: Run}

\begin{codebox}[Launch Simulation]
\begin{lstlisting}[style=bash,numbers=none]
mpirun -np 12 ./ramses_final3d namelist.nml 2>&1 | tee run.log
\end{lstlisting}
\end{codebox}

\section{Key Parameters Quick Reference}

\begin{center}
\begin{longtable}{L{3.8cm} C{2.5cm} L{6.5cm}}
\toprule
\textbf{Parameter} & \textbf{Recommended} & \textbf{Purpose} \\
\midrule
\endhead
\texttt{ordering} & \texttt{'ksection'} & Hierarchical domain decomposition \\
\texttt{memory\_balance} & \texttt{.true.} & Memory-weighted load balancing \\
\texttt{nremap} & \texttt{5} & Load balance frequency (optimal) \\
\midrule
\texttt{ivar\_refine} & \texttt{11} (=NVAR) & Passive scalar index for zoom mask \\
\texttt{var\_cut\_refine} & \texttt{0.01} & Threshold for zoom scalar refinement \\
\texttt{mass\_cut\_refine} & see table & Particle mass filter for \texttt{cpu\_map2} \\
\midrule
\texttt{ngridtot} & see sizing & Total grids; keep virtual mem $<$ CommitLimit \\
\texttt{nparttot} & see sizing & Total particles across all ranks \\
\texttt{levelmin} & IC levelmin & Must match MUSIC base level \\
\texttt{levelmax} & 14--20 & Maximum AMR refinement \\
\bottomrule
\end{longtable}
\end{center}

\subsection{mass\_cut\_refine Reference}

The \texttt{mass\_cut\_refine} parameter filters out heavy (background) dark matter particles from the \texttt{cpu\_map2} density computation in \texttt{rho\_fine}. Set it to a value between the zoom-region particle mass and the coarser-level particle mass. Recommended values by IC finest level:

\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{IC finest level} & \textbf{Zoom $m_{\text{DM}}$} & \textbf{Coarse $m_{\text{DM}}$} & \textbf{mass\_cut\_refine} \\
\midrule
9 & $\sim 4.6 \times 10^7$ & $\sim 3.7 \times 10^8$ & $1.0 \times 10^8$ \\
10 & $\sim 5.8 \times 10^6$ & $\sim 4.6 \times 10^7$ & $1.5 \times 10^7$ \\
11 & $\sim 7.2 \times 10^5$ & $\sim 5.8 \times 10^6$ & $2.0 \times 10^6$ \\
12 & $\sim 9.0 \times 10^4$ & $\sim 7.2 \times 10^5$ & $2.5 \times 10^5$ \\
13 & $\sim 1.1 \times 10^4$ & $\sim 9.0 \times 10^4$ & $3.0 \times 10^4$ \\
\bottomrule
\end{tabular}
\end{center}

\begin{infobox}[Units]
Masses are in $M_\odot/h$ for a 10\,$h^{-1}$\,Mpc box with Planck 2018 cosmology ($\Omega_m=0.3111$, $\Omega_b=0.04$, $h=0.6766$). The exact values depend on the box size and cosmological parameters.
\end{infobox}

\section{Troubleshooting}

\begin{center}
\begin{longtable}{L{4.5cm} L{8.5cm}}
\toprule
\textbf{Error / Symptom} & \textbf{Solution} \\
\midrule
\endhead
\texttt{No more free memory} & Increase \texttt{ngridtot} (or \texttt{ngridmax}) in \texttt{\&AMR\_PARAMS}. \\
\midrule
\texttt{No more free memory for particles} & Increase \texttt{nparttot} (or \texttt{npartmax}) in \texttt{\&AMR\_PARAMS}. \\
\midrule
Process killed (SIGNAL 9, OOM) & \texttt{ngridtot} is too large for available RAM. RAMSES allocates the full array at startup (virtual memory). Check \texttt{CommitLimit} in \texttt{/proc/meminfo} and reduce \texttt{ngridtot} so that total virtual memory stays below it. \\
\midrule
Background AMR explosion (entire box refined, memory exhausted) & Verify \texttt{ivar\_refine=11} and that \texttt{ic\_pvar\_00006} files exist in each IC level directory. Without the zoom geometry scalar, \texttt{cpu\_map2} marks the entire domain for refinement. \\
\midrule
Refinement does not extend beyond IC levels & Check that \texttt{m\_refine} has enough entries (one per extra level above IC finest) and that \texttt{mass\_cut\_refine} is set correctly to exclude heavy background particles. \\
\midrule
Very slow Poisson solver & Increase \texttt{cg\_levelmin} in \texttt{\&POISSON\_PARAMS} to switch from multigrid to conjugate-gradient at the finest levels. Typical: \texttt{cg\_levelmin=14}. \\
\bottomrule
\end{longtable}
\end{center}


%======================================================
\chapter{K-Section Ordering}
%======================================================

\section{Overview}

The \textbf{k-section ordering} replaces the standard Hilbert curve domain decomposition with a hierarchical spatial bisection tree. Unlike the Hilbert curve approach that relies on a 1D space-filling curve, the k-section ordering partitions the 3D domain using recursive bisection along each coordinate axis, producing a balanced k-ary tree of spatial subdomains.

\begin{infobox}[Key Advantage]
The k-section tree enables \textbf{hierarchical MPI exchange} where communication follows the tree structure, achieving $O(\sum k_l)$ messages per exchange instead of $O(N_{\text{cpu}})$ all-to-all communication.
\end{infobox}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{domain_decomposition_improved.png}
\caption{Isometric view of k-section domain decomposition with a $3\times3\times2=18$ uniform partition. Each subbox surface shows the projected column density $\log_{10}(1+\Sigma)$ from a $4096^3$ TSC density field. Colors encode spatial position: R$\to x$, G$\to y$, B$\to z$, providing a smooth gradient across adjacent subboxes.}
\label{fig:ksection_domains}
\end{figure}

\section{Hierarchical MPI Exchange}

Two exchange routines are provided in \texttt{patch/cuda/ksection.f90}:

\begin{itemize}[leftmargin=2em]
  \item \texttt{ksection\_exchange\_dp} --- exclusive exchange (each item $\to$ exactly 1 destination CPU)
  \item \texttt{ksection\_exchange\_dp\_overlap} --- overlap exchange (items routed by spatial bounding box)
\end{itemize}

\subsection{Exclusive Exchange}

Each data item has a known destination CPU. The algorithm performs level-by-level correspondent exchange through the k-section tree, using MPI tags 100--300+level.

\subsection{Overlap Exchange}

Items have spatial extent (bounding box) and may need to reach multiple CPUs. The tree walk determines all destination CPUs whose spatial domain overlaps the item's bounding box. MPI tags 400--500+level are used.

\subsection{Periodic Boundary Conditions}

The optional \texttt{periodic=.true.} parameter enables handling of items that wrap around the domain boundary $[0, \text{scale}]$. For each wrapping dimension, $2^{n_{\text{wrap}}}-1$ shifted copies are generated via bitmask subset enumeration before the tree walk.

\section{Tree Navigation}

The arrays \texttt{ksec\_cpumin/cpumax} and \texttt{ksec\_cpu\_path} (set in \texttt{build\_ksection} and at restart via \texttt{rebuild\_ksec\_cpuranges}) enable each CPU to navigate the tree efficiently.

\section{Verification}

The exchange routines are tested in \texttt{test\_ksection\_exchange} with 4 test cases:
\begin{enumerate}
  \item Exclusive point exchange
  \item Overlap point exchange
  \item Full overlap exchange
  \item Periodic overlap exchange (20\% radius items)
\end{enumerate}
All tests pass.


%======================================================
\chapter{Ghost Zone Exchange via K-Section}
%======================================================

\section{AMR Ghost Zones}

The standard RAMSES ghost zone exchange uses \texttt{MPI\_ISEND/IRECV} with all-to-all communication patterns. This was replaced with k-section tree-routed exchange for the \texttt{ordering='ksection'} mode.

\subsection{Modified File}

\texttt{patch/cuda/virtual\_boundaries.kjhan.f90}

\subsection{Subroutines}

Four k-section variants were implemented:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Subroutine} & \textbf{Direction} & \textbf{Data Type} \\
\midrule
\texttt{make\_virtual\_fine\_dp\_ksec} & Forward & \texttt{real(dp)} \\
\texttt{make\_virtual\_fine\_int\_ksec} & Forward & \texttt{integer} \\
\texttt{make\_virtual\_reverse\_dp\_ksec} & Reverse (+= accumulate) & \texttt{real(dp)} \\
\texttt{make\_virtual\_reverse\_int\_ksec} & Reverse (+= accumulate) & \texttt{integer} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Data Packing}

Each emission grid is packed as:
\[
\texttt{sendbuf}(1{:}\text{twotondim}+2,\; i) = \bigl[\underbrace{c_1, c_2, \ldots, c_8}_{\text{cell data}},\; \underbrace{\text{myid}}_{\text{sender}},\; \underbrace{i}_{\text{index}}\bigr]
\]

The metadata (sender ID, emission index) allows the receiver to scatter data to the correct reception grid without requiring a priori knowledge of the communication pattern.

\subsection{Dispatch}

Dispatch is automatic via:
\begin{lstlisting}[style=fortran,numbers=none]
if(ordering=='ksection') then
   call make_virtual_fine_dp_ksec(xx, ilevel)
   return
end if
\end{lstlisting}

\subsection{Bulk Exchange}

Four bulk variants exchange all columns of a 2D array (e.g., \texttt{uold}, \texttt{f}, \texttt{unew}) in a single \texttt{ksection\_exchange\_dp} call:

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Subroutine} & \textbf{Description} \\
\midrule
\texttt{make\_virtual\_fine\_dp\_bulk} & Forward bulk exchange \\
\texttt{make\_virtual\_fine\_dp\_bulk\_ksec} & Forward bulk (ksection impl.) \\
\texttt{make\_virtual\_reverse\_dp\_bulk} & Reverse bulk exchange \\
\texttt{make\_virtual\_reverse\_dp\_bulk\_ksec} & Reverse bulk (ksection impl.) \\
\bottomrule
\end{tabular}
\end{center}

Buffer layout for \texttt{ncols} variables:
\[
\texttt{sendbuf}\bigl((v{-}1) \cdot 2^{3} + j,\;\text{idx}\bigr) = \texttt{xx}(\text{icell},\; v) \quad v=1{\ldots}\text{ncols},\; j=1{\ldots}8
\]
plus 2 metadata words (sender ID, index). This reduces MPI exchanges from $N_{\text{var}}$ per level to 1 per array.

\section{build\_comm via K-Section}

The \texttt{build\_comm} subroutine's \texttt{MPI\_ALLTOALL} + \texttt{MPI\_ISEND/IRECV} pattern was also replaced with \texttt{ksection\_exchange\_dp}.

\section{MPI\_ALLTOALL Replacements}

Additional \texttt{MPI\_ALLTOALL} calls in the following files were replaced with k-section exchange:
\begin{itemize}[leftmargin=2em]
  \item \texttt{particle\_tree.kjhan.f90}
  \item \texttt{init\_part.f90}
  \item \texttt{multigrid\_fine\_commons.f90} (in \texttt{build\_parent\_comms\_mg})
\end{itemize}

\section{Pre-Allocated Buffer Pool}

Per-level small arrays (child count, peer list, MPI requests) were converted to \texttt{save} variables to eliminate ${\sim}100$ allocations/deallocations per call. The \texttt{peer\_recv} buffer uses grow-only capacity, and the first dimension must exactly match \texttt{nprops} for MPI stride correctness.


%======================================================
\chapter{Multigrid Poisson K-Section Communication}
%======================================================

\section{Motivation}

The multigrid Poisson solver (\texttt{poisson-mg}) accounts for 29--41\% of total runtime. The original MPI communication used \texttt{MPI\_ISEND/IRECV} with all-to-all patterns across all CPUs.

\section{Implementation}

Four k-section variants were added to \texttt{patch/cuda/multigrid\_fine\_commons.f90}:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Subroutine} & \textbf{Direction} & \textbf{Data Type} \\
\midrule
\texttt{make\_virtual\_mg\_dp\_ksec} & Forward & \texttt{real(dp)} \\
\texttt{make\_virtual\_mg\_int\_ksec} & Forward & \texttt{integer} \\
\texttt{make\_reverse\_mg\_dp\_ksec} & Reverse (+= accumulate) & \texttt{real(dp)} \\
\texttt{make\_reverse\_mg\_int\_ksec} & Reverse (+= accumulate) & \texttt{integer} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Key Differences from AMR Exchange}

The MG communication uses different data structures from the standard AMR exchange:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{AMR} & \textbf{Multigrid} \\
\midrule
Active grids & \texttt{active(ilevel)} & \texttt{active\_mg(myid,ilevel)} \\
Reception & \texttt{reception(icpu,ilevel)} & \texttt{active\_mg(icpu,ilevel)} \\
Emission & \texttt{emission(icpu,ilevel)} & \texttt{emission\_mg(icpu,ilevel)} \\
Data arrays & \texttt{xx(igrid+iskip)} & \texttt{active\_mg\%u(icell,ivar)} \\
Indexing & Global grid index & Local offset in active\_mg \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Forward Exchange}

\begin{enumerate}
  \item Pack: for each \texttt{emission\_mg(icpu)\%igrid(i)}, gather \texttt{twotondim} values from \texttt{active\_mg(myid)\%u} + metadata (sender\_id, index)
  \item Exchange via \texttt{ksection\_exchange\_dp}
  \item Scatter: use metadata to write into \texttt{active\_mg(sender)\%u(ridx + step, ivar)}
\end{enumerate}

\subsection{Reverse Exchange (Accumulation)}

\begin{enumerate}
  \item Pack: for each remote \texttt{active\_mg(icpu)\%u(i + step, ivar)} + metadata
  \item Exchange via \texttt{ksection\_exchange\_dp}
  \item Accumulate: \texttt{active\_mg(myid)\%u(emission\_mg(sender)\%igrid(ridx) + step, ivar) += recvbuf}
\end{enumerate}


%======================================================
\chapter{Morton Key Octree}
%======================================================

\section{Overview}

The \texttt{nbor} array (6 neighbor pointers per grid) was replaced with a Morton key hash table for $O(1)$ neighbor lookup. This saves $6 \times 8 \times N_{\text{gridmax}}$ bytes of memory.

\subsection{Modified Files}

\begin{itemize}[leftmargin=2em]
  \item \texttt{patch/oct\_tree/morton\_keys.f90} --- Morton key computation
  \item \texttt{patch/oct\_tree/morton\_hash.f90} --- Hash table and helper functions
  \item \texttt{patch/oct\_tree/morton\_init.f90} --- Initialization and verification
  \item \texttt{patch/oct\_tree/refine\_utils.f90} --- Hash table maintenance
  \item \texttt{patch/oct\_tree/nbors\_utils.kjhan.f90} --- Neighbor lookup functions
\end{itemize}

\section{Morton Key}

A 64-bit Morton key is computed by interleaving the 3D integer coordinates (21 bits per dimension):
\[
\text{key} = \text{interleave}\bigl(\lfloor x_g \cdot 2^{l-1} \rfloor,\; \lfloor y_g \cdot 2^{l-1} \rfloor,\; \lfloor z_g \cdot 2^{l-1} \rfloor\bigr)
\]

\section{Hash Table}

A per-level open-addressing hash table with linear probing and power-of-2 capacity maps Morton keys to grid indices. The table is maintained in \texttt{make\_grid\_coarse/fine} and \texttt{kill\_grid}, with a full rebuild after each time step.

\section{Neighbor Lookup}

Two helper functions in the \texttt{morton\_hash} module:
\begin{itemize}[leftmargin=2em]
  \item \texttt{morton\_nbor\_grid(igrid, ilevel, j)} --- returns \texttt{son(nbor(igrid,j))} equivalent
  \item \texttt{morton\_nbor\_cell(igrid, ilevel, j)} --- returns \texttt{nbor(igrid,j)} equivalent
\end{itemize}

Direction convention: $j = 1{:}{-x},\; 2{:}{+x},\; 3{:}{-y},\; 4{:}{+y},\; 5{:}{-z},\; 6{:}{+z}$.

\section{nbor Array Removal (Phase 4)}

The \texttt{nbor} array is allocated as \texttt{allocate(nbor(1:1,1:1))} (minimum size to avoid compilation errors). All code paths use Morton lookup exclusively.


%======================================================
\chapter{Memory-Based Load Balancing}
%======================================================

\section{Motivation}

Standard RAMSES load balancing distributes cells evenly across CPUs, but cells with many particles consume significantly more memory. Memory-based balancing weights each cell by its memory footprint.

\section{Cost Function}

\[
\text{cell\_cost} = \frac{\texttt{mem\_weight\_grid}}{\text{twotondim}} + \text{numbp}(\text{igrid}) \times \frac{\texttt{mem\_weight\_part}}{\text{twotondim}}
\]

where:
\begin{itemize}[leftmargin=2em]
  \item \texttt{mem\_weight\_grid} = 270 (default) --- memory per grid in dp-equivalents
  \item \texttt{mem\_weight\_part} = 12 (default) --- memory per particle in dp-equivalents
\end{itemize}

\section{Implementation Details}

\begin{itemize}[leftmargin=2em]
  \item All histogram variables use 64-bit integers (\texttt{integer(i8b)}) with \texttt{MPI\_INTEGER8}
  \item \texttt{numbp} is synchronized for virtual/reception grids before cost computation, then restored afterwards
  \item The \texttt{numbp} restore uses a save/restore pattern to avoid breaking the particle tree
\end{itemize}

\section{Parameters}

Controlled by three namelist parameters in \texttt{\&RUN\_PARAMS}:
\begin{itemize}[leftmargin=2em]
  \item \texttt{memory\_balance = .true.} --- enable memory-based balancing
  \item \texttt{mem\_weight\_grid = 270} --- grid memory weight
  \item \texttt{mem\_weight\_part = 12} --- particle memory weight
\end{itemize}


%======================================================
\chapter{Memory Savings: Large Array Optimization}
%======================================================

\section{Overview}

Several large arrays were eliminated or converted to on-demand allocation to reduce steady-state memory usage by ${\sim}960$\,MB (for \texttt{ngridmax}=5M).

\begin{center}
\begin{tabular}{llr}
\toprule
\textbf{Array} & \textbf{Strategy} & \textbf{Savings} \\
\midrule
\texttt{hilbert\_key} & \texttt{allocate(1:1)} for ksection & ${\sim}640$\,MB \\
\texttt{bisec\_ind\_cell} + \texttt{cell\_level} & On-demand alloc/dealloc & ${\sim}320$\,MB \\
\texttt{defrag\_map} & Local scratch during defrag & minor \\
\texttt{nbor} & \texttt{allocate(1:1,1:1)} (Morton) & ${\sim}240$\,MB \\
\bottomrule
\end{tabular}
\end{center}


%======================================================
\chapter{IC Reading with Stream Access}
%======================================================

\section{Motivation}

Sequential Fortran I/O requires reading all preceding planes to reach a target plane, which is $O(n^2)$ for large files. Stream access enables direct byte-offset seeks.

\section{Implementation}

Fortran 2003 \texttt{ACCESS='STREAM'} is used with computed byte offsets:

\begin{lstlisting}[style=fortran,numbers=none]
hdr_bytes = 52 + (i3-1)*plane_bytes + 5
plane_bytes = n1*n2*4 + 8  ! data + 2 record markers
\end{lstlisting}

Applied to hydro IC (deltab, velocity, temperature), particle velocity and position files. Only for \texttt{multiple=.false.} mode.

\subsection{Modified Files}
\begin{itemize}[leftmargin=2em]
  \item \texttt{patch/Horizon5-master-2/init\_flow\_fine.f90}
  \item \texttt{init\_part.f90}
\end{itemize}


%======================================================
\chapter{Load Balance Profiling and Tuning}
%======================================================

\section{Internal Timing}

Detailed timing breakdown was added to \texttt{load\_balance} in \texttt{patch/cuda/load\_balance.kjhan.f90}:

\begin{center}
\begin{tabular}{llr}
\toprule
\textbf{Section} & \textbf{Description} & \textbf{Typical (s/step)} \\
\midrule
\texttt{numbp\_sync} & MPI sync of numbp for virtual grids & 0.8--1.0 \\
\texttt{cmp\_new\_cpu\_map} & Build ksection + compute new map & 0.4--0.6 \\
\texttt{expand\_pass} & build\_comm + make\_virtual loop & 0.8--1.5 \\
\texttt{grid\_migration} & Linked-list reconnection & $< 0.01$ \\
\texttt{allreduce+cpumap\_update} & MPI\_ALLREDUCE $\times$4 + cpu\_map & 2.3--3.3 \\
\texttt{shrink\_pass} & flag\_fine + build\_comm loop & 0.4--1.0 \\
\bottomrule
\end{tabular}
\end{center}

\section{nremap Tuning}

The \texttt{nremap} parameter controls load balancing frequency (every $N$ coarse steps). Testing with 200M particles on 12 ranks showed:

\begin{center}
\begin{tabular}{rrrrl}
\toprule
\textbf{nremap} & \textbf{Total (s)} & \textbf{Loadbal (s)} & \textbf{Speedup} & \textbf{Note} \\
\midrule
1 & 303.8 & 64.4 (21.2\%) & --- & Baseline \\
3 & 269.9 & 24.7 (9.1\%) & 1.13$\times$ & \\
5 & 249.8 & 15.7 (6.3\%) & \textbf{1.22$\times$} & \textbf{Optimal} \\
10 & 258.6 & 11.6 (4.5\%) & 1.17$\times$ & Imbalance grows \\
\bottomrule
\end{tabular}
\end{center}

\begin{infobox}[Default Setting]
\texttt{nremap=5} is set as the default. All four configurations produce \textbf{bit-identical} results: \texttt{econs=3.77E-03}, \texttt{epot=-1.88E-06}, \texttt{ekin=1.23E-06} at step 10.
\end{infobox}

\section{Min/Max Memory Reporting}

The \texttt{writemem\_minmax} subroutine prints per-step min/max memory usage across all MPI ranks.


%======================================================
\chapter{Zoom-In Simulation Setup}
\label{ch:zoomin}
%======================================================

\section{Overview}

This chapter describes how to configure and run a cosmological zoom-in simulation with cuRAMSES, targeting ${\sim}1$\,kpc physical resolution within a selected sub-region of a larger cosmological volume. The setup uses:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Box size}: 10\,$h^{-1}$\,Mpc (comoving)
  \item \textbf{Zoom region}: ${\sim}1.25$\,$h^{-1}$\,Mpc (centered, with padding from MUSIC)
  \item \textbf{IC levels}: 7--11 (generated by MUSIC), corresponding to base $128^3$ up to effective $2048^3$
  \item \textbf{AMR levels}: up to 14 (adaptive refinement beyond IC levels)
  \item \textbf{Physics}: hydrodynamics, gravity, radiative cooling, star formation, and AGN feedback
\end{itemize}

The main challenge in zoom-in simulations is preventing the AMR machinery from refining the entire box (which would exhaust memory). This is solved by using a passive scalar variable as a \emph{zoom geometry mask} that restricts refinement to the zoom region during initialization.


\section{Initial Conditions}

\subsection{MUSIC Configuration}

Initial conditions are generated by the MUSIC code with a multi-level zoom setup. The MUSIC configuration file specifies the base resolution, zoom region, and cosmological parameters.

\begin{codebox}[zoomin\_10Mpc.conf]
\begin{lstlisting}[style=bash,numbers=none]
[setup]
boxlength       = 10          # Box size in Mpc/h
zstart          = 50          # Starting redshift
levelmin        = 7           # Base grid: 2^7 = 128^3
levelmin_TF     = 7           # Transfer function grid
levelmax        = 11          # Zoom finest: 2^11 = 2048^3 effective
padding         = 8
overlap         = 4
ref_center      = 0.5, 0.5, 0.5
ref_extent      = 0.04, 0.04, 0.04
align_top       = yes
baryons         = yes
use_2LPT        = yes
periodic_TF     = yes

[cosmology]
# Planck 2018 (TT,TE,EE+lowE+lensing)
Omega_m         = 0.3111
Omega_L         = 0.6889
Omega_b         = 0.04
H0              = 67.66
sigma_8         = 0.8102
nspec           = 0.9665
transfer        = eisenstein

[output]
format          = grafic2
filename        = IC_zoomin
\end{lstlisting}
\end{codebox}

\subsection{Generated IC Structure}

MUSIC produces a directory hierarchy with one sub-directory per refinement level:

\begin{center}
\begin{tabular}{cccl}
\toprule
\textbf{Level} & \textbf{Grid Size} & \textbf{Resolution} & \textbf{Coverage} \\
\midrule
7 & $128^3$ & ${\sim}78$\,kpc/$h$ & Entire box \\
8 & variable & ${\sim}39$\,kpc/$h$ & Zoom region + padding \\
9 & variable & ${\sim}20$\,kpc/$h$ & Zoom region + padding \\
10 & variable & ${\sim}10$\,kpc/$h$ & Zoom region + padding \\
11 & variable & ${\sim}5$\,kpc/$h$ & Zoom region (finest IC) \\
\bottomrule
\end{tabular}
\end{center}

Each level directory contains GRAFIC2 binary files: \texttt{ic\_deltab} (baryon overdensity), \texttt{ic\_velcx/y/z} (baryon velocities), \texttt{ic\_velcdx/y/z} (dark matter velocities), \texttt{ic\_poscdx/y/z} (dark matter position offsets), and \texttt{ic\_refmap} (refinement map, level~7 only).


\section{Zoom Geometry Scalar (\texttt{ic\_pvar\_00006})}

\begin{important}
This is the single most critical aspect of the zoom-in setup. Incorrect configuration of the zoom geometry scalar will cause the entire box to be refined, leading to immediate memory exhaustion.
\end{important}

\subsection{The Problem}

In a standard RAMSES simulation, the \texttt{init\_refmap} subroutine (called when \texttt{ivar\_refine=0}) reads the \texttt{ic\_refmap} file and sets \texttt{cpu\_map2=1} for cells that should be refined. However, the default \texttt{flag\_utils} logic using \texttt{cpu\_map2} applies refinement uniformly wherever the map is set, without distinguishing between zoom and background regions at higher AMR levels. This causes the entire box to be recursively refined, rapidly exhausting memory.

\subsection{The Solution: Passive Scalar Mask}

The solution uses the 6th passive scalar variable (\texttt{ic\_pvar\_00006}) as a zoom geometry indicator. With \texttt{NVAR=11} (5~hydro + 1~metal + 5~passive), the 6th passive scalar corresponds to hydro variable index 11. Setting \texttt{ivar\_refine=11} activates a different refinement criterion in \texttt{flag\_utils}:

\begin{lstlisting}[style=fortran,numbers=none]
! In flag_utils.kjhan.f90 (during init):
if(ivar_refine > 0) then
   do i=1,ncell
      ok(i) = ok(i) .or. &
           (uold(ind_cell(i),ivar_refine) / uold(ind_cell(i),1) &
            > var_cut_refine)
   end do
end if
\end{lstlisting}

This checks whether the passive scalar (divided by density for the conserved-to-primitive conversion) exceeds \texttt{var\_cut\_refine} (typically 0.01). Cells in the zoom region have a scalar value of 1.0 and pass the test; background cells have 0.0 and are skipped.

\subsection{How It Works at Each Stage}

\begin{enumerate}
  \item \textbf{During initialization} (\texttt{init=.true.}): When \texttt{ivar\_refine} is nonzero, \texttt{init\_refmap} is \emph{not} called (line~31 of \texttt{amr/init\_refine.f90}):
\begin{lstlisting}[style=fortran,numbers=none]
if(ivar_refine==0) call init_refmap
\end{lstlisting}
  Instead, \texttt{init\_flow} loads the hydro IC (including \texttt{ic\_pvar\_00006}) and the refinement flag is set by the passive scalar criterion: $\texttt{uold}(\text{cell},11)/\texttt{uold}(\text{cell},1) > 0.01$.

  \item \textbf{After initialization} (\texttt{init=.false.}): The refinement criterion switches to the standard density-based Lagrangian criterion (\texttt{m\_refine}). The \texttt{cpu\_map2} array is set by \texttt{rho\_fine} with the \texttt{mass\_cut\_refine} parameter filtering out heavy background dark matter particles, ensuring that only the zoom region (populated by light, high-resolution particles) triggers further refinement.
\end{enumerate}

\subsection{Creating \texttt{ic\_pvar\_00006}}

The Python script \texttt{test\_ksection/create\_pvar006.py} generates the zoom geometry scalar for each IC level:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Level 7} (base): Reads \texttt{ic\_refmap} from the level~7 directory. Nonzero entries (zoom cells) are set to 1.0; zero entries (background) are set to 0.0. The result is written in GRAFIC2 format as \texttt{ic\_pvar\_00006}.
  \item \textbf{Levels 8+} (zoom sub-levels): All cells are set to 1.0, since the entire grid at these levels lies within the zoom region by construction.
\end{itemize}

\begin{codebox}[create\_pvar006.py (core logic)]
\begin{lstlisting}[style=bash,numbers=none]
for level in LEVELS:
    if level == base_level:
        # Read ic_refmap, convert: nonzero -> 1.0, zero -> 0.0
        refmap = read_grafic2_data(refmap_file, n1, n2, n3)
        pvar = np.where(refmap != 0, 1.0, 0.0).astype(np.float32)
    else:
        # Zoom sub-levels: all cells = 1.0
        pvar = np.ones((n3, n2, n1), dtype=np.float32)
    write_grafic2(pvar_file, header_bytes, pvar)
\end{lstlisting}
\end{codebox}

\begin{infobox}[Editing the Script]
Before running, edit the \texttt{IC\_DIR} variable in \texttt{create\_pvar006.py} to point to your IC directory, and adjust \texttt{LEVELS} to match the levels generated by MUSIC.
\end{infobox}

\subsection{mass\_cut\_refine}

After initialization, \texttt{rho\_fine} computes the density field used to set \texttt{cpu\_map2} for refinement flagging. The \texttt{mass\_cut\_refine} parameter acts as a mass filter:

\begin{lstlisting}[style=fortran,numbers=none]
! In rho_fine.f90:
if(mass_cut_refine > 0.0) then
   do j = 1, np
      if(ttt(j) == 0d0) then          ! dark matter only
         ok(j) = ok(j) .and. mmm(j) < mass_cut_refine
      endif
   end do
endif
\end{lstlisting}

Particles heavier than \texttt{mass\_cut\_refine} are excluded from the density computation, so only high-resolution zoom particles contribute to the refinement map. Set \texttt{mass\_cut\_refine} to a value between the zoom-region DM particle mass and the next-coarser level's DM particle mass. Refer to the table in Chapter~\ref{ch:quickstart} for recommended values.


\section{Memory Considerations}

\subsection{ngridtot Sizing}

RAMSES allocates all grid-related arrays at startup based on \texttt{ngridmax} (per-CPU maximum grids). When \texttt{ngridtot} is specified, \texttt{ngridmax} is computed as \texttt{ngridtot/ncpu}. The critical constraint is:

\begin{important}
The \textbf{total virtual memory} allocated by all MPI ranks on a node must not exceed the kernel's \texttt{CommitLimit}. Exceeding this causes the OOM killer to terminate the process (SIGNAL~9).
\end{important}

\subsection{CommitLimit}

The Linux kernel's \texttt{CommitLimit} determines the maximum total virtual memory that can be allocated:
\[
\texttt{CommitLimit} = \texttt{RAM} \times \frac{\texttt{overcommit\_ratio}}{100} + \texttt{swap}
\]

Check the current value:
\begin{codebox}[Check CommitLimit]
\begin{lstlisting}[style=bash,numbers=none]
grep CommitLimit /proc/meminfo
\end{lstlisting}
\end{codebox}

On systems without swap (common for HPC nodes), \texttt{CommitLimit = RAM $\times$ overcommit\_ratio/100}. The default \texttt{overcommit\_ratio} is 50, so a 256\,GB node has ${\sim}128$\,GB CommitLimit.

\subsection{Virtual vs. Physical Memory}

RAMSES allocates the full \texttt{ngridmax}-sized arrays at startup, consuming virtual memory immediately. Physical (resident) memory grows as grids are actually created during the simulation. The key arrays that dominate virtual memory usage are:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Array} & \textbf{Size} & \textbf{Per-grid bytes} \\
\midrule
\texttt{uold(1:ncell,1:nvar)} & $8 \times 8 \times \text{nvar}$ & $8 \times 8 \times 11 = 704$ \\
\texttt{unew(1:ncell,1:nvar)} & same & 704 \\
\texttt{f(1:ncell,1:3)} & $8 \times 8 \times 3$ & 192 \\
\texttt{son/father/next/prev} & $8 \times 8$ each & $4 \times 64 = 256$ \\
\texttt{phi/rho} & $8 \times 8$ each & $2 \times 64 = 128$ \\
\texttt{flag1/flag2/cpu\_map/map2} & $4 \times 8$ each & $4 \times 32 = 128$ \\
\bottomrule
\end{tabular}
\end{center}

\begin{remark}[Rule of Thumb]
As a rough estimate, each grid (oct = 8 cells) requires ${\sim}2$--$3$\,kB of virtual memory across all arrays. For \texttt{ngridmax}=25\,million (i.e., \texttt{ngridtot}=200M with 8~CPUs), each rank allocates ${\sim}50$--75\,GB of virtual memory. Ensure that \texttt{ncpu $\times$ per\_rank\_virtual $<$ CommitLimit}.
\end{remark}


\section{Poisson MG Fine-Level Optimization}

The multigrid Poisson solver (\texttt{poisson-mg}) is typically the single largest time consumer, accounting for 29--55\% of total runtime. Several optimizations were applied to the fine-level V-cycle in \texttt{poisson/multigrid\_fine\_fine.kjhan.f90}:

\subsection{Precomputed Neighbor Grid Array}

Before entering the V-cycle iteration loop, all 6-directional neighbor grids are precomputed and stored in a contiguous array:

\begin{lstlisting}[style=fortran,numbers=none]
! nbor_grid_fine(0:twondim, 1:ngrid)
!   index 0 = self (igrid_amr)
!   index 1..6 = neighbor grids in -x,+x,-y,+y,-z,+z
call precompute_nbor_grid_fine(ilevel)
\end{lstlisting}

This replaces per-cell \texttt{morton\_nbor\_grid} calls inside the Gauss-Seidel and residual loops with simple array lookups, eliminating hash table probes from the innermost loops.

\subsection{Merged Red-Black Gauss-Seidel Exchange}

The standard multigrid implementation performs a ghost zone exchange after each half-sweep of the red-black Gauss-Seidel smoother:
\begin{center}
\texttt{red} $\to$ \texttt{exchange} $\to$ \texttt{black} $\to$ \texttt{exchange}
\end{center}

This was simplified to:
\begin{center}
\texttt{red} $\to$ \texttt{black} $\to$ \texttt{exchange}
\end{center}

The black sweep uses slightly stale ghost values from the red sweep (``chaotic relaxation''), which does not affect multigrid convergence. This reduces the number of MPI exchanges per iteration from 9 to 5 (a 44\% reduction in communication calls).

\subsection{Residual and Norm Single Pass}

The residual computation and the $L^2$ norm reduction were fused into a single pass:

\begin{lstlisting}[style=fortran,numbers=none]
! Optional norm2 argument: if present, compute
! both residual and L2 norm in one sweep
call cmp_residual_mg_fine(ilevel, norm2)
\end{lstlisting}

This eliminates a redundant loop over all cells when both the residual and its norm are needed (at the first iteration and after post-smoothing).

\subsection{Division to Multiplication}

In the Gauss-Seidel fast path, the division by \texttt{dtwondim} was replaced with multiplication by a precomputed reciprocal:

\begin{lstlisting}[style=fortran,numbers=none]
real(dp) :: oneoverdtwondim
oneoverdtwondim = 1.0d0 / dble(twondim)
! In loop:
phi = sum_neighbors * oneoverdtwondim   ! was: / dtwondim
\end{lstlisting}

\subsection{Performance Impact}

These optimizations combined reduce the Poisson solver's share of total runtime:

\begin{center}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Before optimization & 55.1\% of runtime \\
After optimization & 38.6\% of runtime \\
MPI exchange reduction & 44\% fewer calls \\
Iteration count & unchanged (Level~8: 5, Level~9: 4) \\
Convergence & verified (econs = 3.79E-03 at step~10) \\
\bottomrule
\end{tabular}
\end{center}

\begin{infobox}[Chaotic Relaxation]
The merged red-black exchange introduces a minor change in the energy conservation value (3.77E-03 $\to$ 3.79E-03) due to the slightly different relaxation path. This is well within the multigrid tolerance and does not affect physical results. The membal and nomembal tests produce identical values.
\end{infobox}


%------------------------------------------------------
% PART II: Namelist Reference
%------------------------------------------------------
\part{Namelist Reference}

%======================================================
\chapter{RUN\_PARAMS}
%======================================================

Runtime control parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{cosmo} & logical & \texttt{.false.} & Enable cosmological simulation \\
\texttt{pic} & logical & \texttt{.false.} & Enable Particle-In-Cell \\
\texttt{poisson} & logical & \texttt{.false.} & Enable Poisson gravity solver \\
\texttt{hydro} & logical & \texttt{.false.} & Enable hydrodynamics \\
\texttt{rt} & logical & \texttt{.false.} & Enable radiative transfer \\
\texttt{sink} & logical & \texttt{.false.} & Enable sink particles \\
\texttt{verbose} & logical & \texttt{.false.} & Verbose output \\
\texttt{debug} & logical & \texttt{.false.} & Debug mode \\
\midrule
\texttt{nrestart} & integer & 0 & Restart file number (0 = new run) \\
\texttt{nstepmax} & integer & 1000000 & Maximum number of coarse steps \\
\texttt{ncontrol} & integer & 1 & Frequency of control variable output \\
\texttt{nsubcycle} & integer[] & 2 & Subcycling factor per level \\
\texttt{nremap} & integer & \textbf{5} & Load balancing frequency (0=never) \\
\texttt{ordering} & char & \texttt{hilbert} & Domain decomposition: \texttt{hilbert}, \texttt{bisection}, \texttt{ksection} \\
\texttt{static} & logical & \texttt{.false.} & Static (no refinement) mode \\
\texttt{overload} & integer & 1 & MPI overload factor \\
\texttt{cost\_weighting} & logical & \texttt{.true.} & CPU time-based cost weighting \\
\midrule
\multicolumn{4}{l}{\textit{Memory-based load balancing (new)}} \\
\midrule
\texttt{memory\_balance} & logical & \texttt{.false.} & Enable memory-weighted balancing \\
\texttt{mem\_weight\_grid} & integer & 270 & Memory per grid (dp-equivalents) \\
\texttt{mem\_weight\_part} & integer & 12 & Memory per particle (dp-equivalents) \\
\bottomrule
\end{longtable}


%======================================================
\chapter{AMR\_PARAMS}
%======================================================

Adaptive Mesh Refinement grid parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{levelmin} & integer & 1 & Minimum (uniform) refinement level \\
\texttt{levelmax} & integer & 1 & Maximum refinement level \\
\texttt{ngridmax} & integer8 & 0 & Max grids per CPU (0 = auto) \\
\texttt{ngridtot} & integer8 & 0 & Total grids for auto-computation \\
\texttt{npartmax} & integer & 0 & Max particles per CPU (0 = auto) \\
\texttt{nparttot} & integer & 0 & Total particles for auto-computation \\
\texttt{nexpand} & integer[] & 1 & Mesh expansion layers per level \\
\texttt{boxlen} & real(dp) & 1.0 & Box side length in code units \\
\bottomrule
\end{longtable}


%======================================================
\chapter{OUTPUT\_PARAMS}
%======================================================

Output control parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{noutput} & integer & 1 & Number of scheduled outputs \\
\texttt{foutput} & integer & 1000000 & Output every $N$ steps \\
\texttt{fbackup} & integer & 1000000 & Backup every $N$ steps \\
\texttt{aout} & real[] & 1.1 & Output expansion factors (cosmo) \\
\texttt{tout} & real[] & 0.0 & Output times (non-cosmo) \\
\texttt{output\_mode} & integer & 0 & Hi-res output mode \\
\texttt{gadget\_output} & logical & \texttt{.false.} & Write Gadget-format snapshots \\
\texttt{walltime\_hrs} & real(dp) & $-1$ & Job walltime (hours, $<0$ = ignore) \\
\texttt{minutes\_dump} & real(dp) & 1.0 & Dump this many minutes before walltime \\
\bottomrule
\end{longtable}


%======================================================
\chapter{INIT\_PARAMS}
%======================================================

Initial conditions parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{filetype} & char(20) & \texttt{ascii} & IC file format: \texttt{ascii}, \texttt{grafic}, \texttt{gadget} \\
\texttt{initfile} & char[] & \texttt{' '} & IC file path per level \\
\texttt{multiple} & logical & \texttt{.false.} & Multiple IC files per rank \\
\texttt{nregion} & integer & 0 & Number of IC regions \\
\bottomrule
\end{longtable}


%======================================================
\chapter{REFINE\_PARAMS}
%======================================================

Refinement criteria parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{m\_refine} & real[] & $-1$ & Lagrangian mass threshold per level \\
\texttt{ivar\_refine} & integer & $-1$ & Variable index for gradient refinement \\
\texttt{var\_cut\_refine} & real(dp) & $-1$ & Variable threshold \\
\texttt{mass\_cut\_refine} & real(dp) & $-1$ & Particle mass threshold \\
\texttt{interpol\_var} & integer & 0 & Interpolation variable (0=conservative, 1=primitive) \\
\texttt{interpol\_type} & integer & 1 & Interpolation type (0=MinMod, 1=MonCen) \\
\texttt{sink\_refine} & logical & \texttt{.false.} & Fully refine around sinks \\
\texttt{jeans\_ncells} & real(dp) & $-1$ & Jeans length in cells ($>0$ enables polytropic EOS) \\
\bottomrule
\end{longtable}


%======================================================
\chapter{HYDRO\_PARAMS}
%======================================================

Hydrodynamics solver parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{gamma} & real(dp) & $\frac{5}{3}$ & Adiabatic index $\gamma$ \\
\texttt{courant\_factor} & real(dp) & 0.8 & Courant--Friedrichs--Lewy number \\
\texttt{scheme} & char(20) & \texttt{muscl} & Hydro scheme (\texttt{muscl}) \\
\texttt{slope\_type} & integer & 1 & Slope limiter (1=MinMod, 2=MonCen, 3=unlimited) \\
\texttt{pressure\_fix} & logical & \texttt{.false.} & Pressure floor for strong shocks \\
\texttt{beta\_fix} & real(dp) & 0.0 & Pressure fix strength \\
\texttt{isothermal} & logical & \texttt{.false.} & Isothermal mode \\
\bottomrule
\end{longtable}


%======================================================
\chapter{POISSON\_PARAMS}
%======================================================

Gravity and Poisson solver parameters.

\begin{longtable}{L{3.5cm} C{1.8cm} C{1.8cm} L{6cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{epsilon} & real(dp) & $10^{-4}$ & Multigrid convergence criterion \\
\texttt{gravity\_type} & integer & 0 & Gravity type (0=self-gravity, $>0$=analytic) \\
\texttt{cg\_levelmin} & integer & 999 & Min level for CG solver fallback \\
\texttt{cic\_levelmax} & integer & 0 & Max level for CIC particle interpolation \\
\bottomrule
\end{longtable}


%======================================================
\chapter{PHYSICS\_PARAMS}
%======================================================

Subgrid physics parameters (star formation, feedback, cooling).

\begin{longtable}{L{3.5cm} C{1.8cm} C{2cm} L{5.5cm}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\endhead
\texttt{cooling} & logical & \texttt{.false.} & Enable radiative cooling \\
\texttt{metal} & logical & \texttt{.false.} & Enable metal tracking \\
\texttt{haardt\_madau} & logical & \texttt{.false.} & UV background \\
\texttt{z\_reion} & real(dp) & 8.5 & Reionization redshift \\
\midrule
\multicolumn{4}{l}{\textit{Star formation}} \\
\midrule
\texttt{n\_star} & real(dp) & 0.1 & SF density threshold (H/cc) \\
\texttt{t\_star} & real(dp) & 0.0 & SF timescale (Gyr) \\
\texttt{eps\_star} & real(dp) & 0.0 & SF efficiency \\
\texttt{T2\_star} & real(dp) & 0.0 & ISM polytropic temperature \\
\texttt{g\_star} & real(dp) & 1.6 & ISM polytropic index \\
\texttt{sf\_birth\_properties} & logical & \texttt{.true.} & Output stellar birth properties \\
\midrule
\multicolumn{4}{l}{\textit{Cosmology (read from IC header or namelist)}} \\
\midrule
\texttt{omega\_b} & real(dp) & 0.0 & Baryon density $\Omega_b$ \\
\texttt{omega\_m} & real(dp) & 1.0 & Matter density $\Omega_m$ \\
\texttt{omega\_l} & real(dp) & 0.0 & Dark energy $\Omega_\Lambda$ \\
\texttt{h0} & real(dp) & 1.0 & Hubble constant $H_0$ (km/s/Mpc) \\
\midrule
\multicolumn{4}{l}{\textit{Feedback}} \\
\midrule
\texttt{f\_ek} & real(dp) & 1.0 & SN kinetic energy fraction \\
\texttt{rbubble} & real(dp) & 0.0 & SN superbubble radius (pc) \\
\texttt{yield\-table\-filename} & char & --- & Yield table file path \\
\bottomrule
\end{longtable}


%======================================================
\chapter{Example Namelist}
%======================================================

\section{Cosmological Simulation with Memory Balancing}

\begin{codebox}[cosmo\_ksection\_membal.nml]
\begin{lstlisting}[style=fortran,numbers=none]
&RUN_PARAMS
cosmo=.true.
pic=.true.
poisson=.true.
hydro=.true.
nrestart=0
nremap=5
nsubcycle=1,1,2
ncontrol=1
nstepmax=10
ordering='ksection'
memory_balance=.true.
/

&OUTPUT_PARAMS
noutput=1
aout=1.0
/

&INIT_PARAMS
filetype='grafic'
initfile(1)='/path/to/ics/level_008'
/

&AMR_PARAMS
levelmin=8
levelmax=10
nexpand=1
ngridtot=40000000
nparttot=200000000
/

&REFINE_PARAMS
m_refine=3*8.,
ivar_refine=0
interpol_var=1
interpol_type=0
/

&HYDRO_PARAMS
gamma=1.6666667
courant_factor=0.8
scheme='muscl'
slope_type=1
/

&POISSON_PARAMS
/

&PHYSICS_PARAMS
sf_birth_properties=.false.
yieldtablefilename='yield_table.asc'
/
\end{lstlisting}
\end{codebox}


%------------------------------------------------------
% PART III: Build and Testing
%------------------------------------------------------
\part{Build and Testing}

%======================================================
\chapter{Build Instructions}
\label{ch:build}
%======================================================

\section{Prerequisites}

\begin{itemize}[leftmargin=2em]
  \item Intel Fortran Compiler (\texttt{ifx}) with MPI support (\texttt{mpiifx})
  \item OpenMP support (\texttt{-qopenmp})
\end{itemize}

\section{Build}

\begin{codebox}[Build Commands]
\begin{lstlisting}[style=bash,numbers=none]
cd bin
make clean
make
\end{lstlisting}
\end{codebox}

The binary is produced as \texttt{bin/ramses\_final3d}.

\section{Compile-Time Options}

Key preprocessor flags set in the Makefile:
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Flag} & \textbf{Meaning} \\
\midrule
\texttt{-DNDIM=3} & Three-dimensional simulation \\
\texttt{-DNVECTOR=32} & Vector length for grid sweeps \\
\texttt{-DLONGINT} & 64-bit integer grid indices \\
\texttt{-DQUADHILBERT} & Quad-precision Hilbert keys \\
\texttt{-DNVAR=11} & Number of hydro variables \\
\texttt{-DNPRE=8} & Passive scalar variables \\
\bottomrule
\end{tabular}
\end{center}


%======================================================
\chapter{Verification Tests}
%======================================================

\section{Test Configuration}

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Setting} & \textbf{Value} \\
\midrule
IC & MUSIC \texttt{level\_008} (GRAFIC2 format) \\
MPI ranks & 12 \\
Levels & 8--10 \\
Steps & 10 \\
Ordering & ksection \\
\bottomrule
\end{tabular}
\end{center}

\section{Reference Values (nremap=5)}

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Test} & \textbf{nparttot} & \textbf{econs} & \textbf{epot} & \textbf{ekin} & \textbf{mcons} \\
\midrule
membal & 200M & 3.77E-03 & $-$1.88E-06 & 1.23E-06 & $-$1.84E-16 \\
nomembal & 80M & 3.77E-03 & $-$1.88E-06 & 1.23E-06 & $-$1.84E-16 \\
\bottomrule
\end{tabular}
\end{center}

\section{Running Tests}

\begin{codebox}[Membal Test]
\begin{lstlisting}[style=bash,numbers=none]
cd test_ksection/run_cosmo_membal
cp ../../bin/ramses_final3d .
mpirun -np 12 ./ramses_final3d \
    ../cosmo_ksection_membal.nml
\end{lstlisting}
\end{codebox}

Verify that at step 10: \texttt{econs=3.77E-03}, \texttt{epot=-1.88E-06}, \texttt{ekin=1.23E-06}.


%------------------------------------------------------
% Appendix
%------------------------------------------------------
\appendix

\chapter{Modified Files Summary}

\begin{longtable}{L{6.5cm} L{7cm}}
\toprule
\textbf{File} & \textbf{Modifications} \\
\midrule
\endhead
\multicolumn{2}{l}{\textit{patch/cuda/}} \\
\midrule
\texttt{amr\_parameters.jaehyun.f90} & Memory balance params, nremap=5 default \\
\texttt{amr\_commons.kjhan.f90} & grid\_level array, communicator type \\
\texttt{read\_params.jaehyun.f90} & Read new namelist params \\
\texttt{bisection.f90} & nc\_in parameter, 64-bit histograms \\
\texttt{ksection.f90} & K-section tree + exchange routines \\
\texttt{load\_balance.kjhan.f90} & numbp sync, bulk exchange, internal timing \\
\texttt{virtual\_boundaries.kjhan.f90} & Ksec ghost exchange + bulk + build\_comm \\
\texttt{multigrid\_fine\_commons.f90} & MG ksection communication \\
\texttt{init\_amr.f90} & Memory savings, Morton init \\
\texttt{update\_time.f90} & Memory reporting \\
\texttt{adaptive\_loop.jaehyun.f90} & writemem\_minmax, Morton rebuild, bulk exchange \\
\midrule
\multicolumn{2}{l}{\textit{patch/oct\_tree/}} \\
\midrule
\texttt{morton\_keys.f90} & Morton key computation \\
\texttt{morton\_hash.f90} & Hash table + neighbor helpers \\
\texttt{morton\_init.f90} & Build and verify \\
\texttt{refine\_utils.f90} & Hash maintenance in grid ops \\
\texttt{nbors\_utils.kjhan.f90} & Morton-based neighbor lookup \\
\midrule
\multicolumn{2}{l}{\textit{patch/Horizon5-master-2/}} \\
\midrule
\texttt{init\_flow\_fine.f90} & Stream access IC reading \\
\texttt{init\_part.f90} & Stream access particle IC \\
\texttt{particle\_tree.kjhan.f90} & MPI\_ALLTOALL $\to$ ksection \\
\texttt{amr\_step.jaehyun.f90} & Bulk virtual exchange \\
\bottomrule
\end{longtable}


\end{document}
